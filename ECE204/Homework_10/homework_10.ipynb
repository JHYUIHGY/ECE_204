{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU5Ko-tF27Kk"
      },
      "source": [
        "# Homework 10\n",
        "## ECE 204 Data Science & Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-Y88smSw27Km"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Add other import statements you need here\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK0-OK4o27Kn"
      },
      "source": [
        "---\n",
        "**Problem 1.** Read in `australia-rosslyn-bay.csv`.\n",
        "\n",
        "**What is the mean air temperature for all measurements made between 2 pm and 6 pm (Both inclusive)?**\n",
        "\n",
        "*Note:* in 24-hour time (which is what this dataset uses, 2pm is hour 14 and 6pm is hour 18)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "DqoKlr7E27Kn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21.59373263605164"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# given code\n",
        "df = pd.read_csv(\"australia_rosslyn_bay.csv\", parse_dates=True, index_col=0)\n",
        "\n",
        "# your code here\n",
        "df.head()\n",
        "\n",
        "new_df = df[(df.index.hour >= 14) & (df.index.hour <= 18)]\n",
        "new_df['air_temp'].mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0hkPD6G27Ko"
      },
      "source": [
        "---\n",
        "**Problem 2.** Read in `australia-rosslyn-bay.csv`. **What is the mean air temperature for all measurements made in October?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8LG0AXQx27Kp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23.13707997641973"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# given code\n",
        "df = pd.read_csv(\"australia_rosslyn_bay.csv\", parse_dates=True, index_col=0)\n",
        "\n",
        "# your code here\n",
        "new_df = df[(df.index.month == 10)]\n",
        "new_df['air_temp'].mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NMHIr1g27Kq"
      },
      "source": [
        "---\n",
        "**Problem 3.** Read in `australia-rosslyn-bay.csv`. **What month has the lowest average pressure?** Report the answer as a number (e.g., \"3\" for \"March\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RnyWCd2P27Kq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1009.8969990974729\n",
            "1010.1980465815176\n",
            "1012.500877770463\n",
            "1016.0871671671672\n",
            "1016.4928805710978\n",
            "1018.7389716100854\n",
            "1019.1519600307455\n",
            "1018.5435132482825\n",
            "1017.57908017908\n",
            "1014.5137355079582\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"australia_rosslyn_bay.csv\", parse_dates=True, index_col=0)\n",
        "\n",
        "# your code here\n",
        "for i in range(1,11):\n",
        "    new_df = df[(df.index.month == i)]\n",
        "    print(new_df['pressure'].mean())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhhYaVUi27Kr"
      },
      "source": [
        "---\n",
        "**Problem 4.** Read in australia-rosslyn-bay.csv. **What is the 95th percentile (or quantile) of the air pressure during August?**\n",
        "\n",
        "*Hint:* https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.quantile.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "7h8jJ38Z27Kr"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1024.0"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"australia_rosslyn_bay.csv\", parse_dates=True, index_col=0)\n",
        "\n",
        "# your code here\n",
        "new_df = df[(df.index.month == 8)]\n",
        "new_df['pressure'].quantile(0.95)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBdaJCnh27Ks"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4px_NnCE27Ks"
      },
      "source": [
        "The following questions are based on a synthetic dataset \"share-prices.csv\" that capture daily mean share prices ((opening + closing)/ 2) of a company from 1st January, 2000 to 2nd May, 2000. This amount to 123 mean share price observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYs0EkL_27Kt"
      },
      "source": [
        "The next two questions will pretend you're a stock market analyst on May 2nd, 2000 and are developing a new method of finding future stock prices. You're using all days up to and including April 19th, 2000 as train data, and trying to predict stock prices on April 20th, 2000 and beyond.\n",
        "\n",
        "Let's pretend we're developing a new technique to predict future performance. This means that there is a natural division between the train and test sets:\n",
        "\n",
        "- **Train set:** January 1st, 2000 (2000-01-01) to 19th April, 2000 (2000-04-19), or 110 days.\n",
        "- **Test set:** April 20th, 2000 (2000-04-20) to the end, 2nd May, 2000 (2000-05-02), or 13 days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF6Za33K27Ku"
      },
      "source": [
        "The next two questions use the library `statsmodels`, and only for the \"auto-regressive\" model or AR model. Auto-regression is linear regression on the last `maxlag` values of a time series (and below the statsmodel package is only used to help with predicting more than one new value).\n",
        "\n",
        "`statsmodels` differs from Scikit-Learn in a couple ways:\n",
        "\n",
        "* In statsmodels, the data is passed to during model creation (not during model fitting like Scikit-Learn).\n",
        "* In statsmodels, the hyper-parameters are passed to during model fitting (not during model creation like Scikit-Learn).\n",
        "\n",
        "Notably, predicting new values only needs the train dataset: the prediction works recursively starting from the last value in the train dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufjF5iOt27Ku"
      },
      "source": [
        "---\n",
        "**Problem 5.** Read in `share-prices.csv`. Train an auto-regressive model with the previous 4 weeks of data, or `lags=28`; that is, predict today's stock price given the last 28 days.\n",
        "\n",
        "Use the historical data (or train set) to fit the model. Predict stock price from April, 20th 2000 to May 2nd, 2000. This will happen recursively using statsmodels – the statsmodel auto-regressive model will generate a prediction for April 20th, then use that value to predict April 21st and so on.\n",
        "\n",
        "**What is the mean squared error between the predicted and true values from April 20th to May 2nd 2000?**\n",
        "\n",
        "*Gold star question:* Would you use this model for actually making decisions on stock transactions? It'd be useful to plot and visualize both the predicted and true values for the testing time-period. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "W_gR75XM27Ku",
        "outputId": "abc001ee-0cd5-4e33-dfd3-f3292753cbdb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2000-04-15</th>\n",
              "      <td>57.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-04-16</th>\n",
              "      <td>42.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-04-17</th>\n",
              "      <td>51.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-04-18</th>\n",
              "      <td>58.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-04-19</th>\n",
              "      <td>64.79</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            price\n",
              "date             \n",
              "2000-04-15  57.24\n",
              "2000-04-16  42.70\n",
              "2000-04-17  51.28\n",
              "2000-04-18  58.82\n",
              "2000-04-19  64.79"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"share_prices.csv\", parse_dates=True, index_col=0)\n",
        "df.index.freq = \"D\"  # set as \"daily\" frequency; observations are recorded every day. Removes a warning in statsmodels\n",
        "\n",
        "# Training and testing set\n",
        "df_train = df.loc[:'2000-04-19']\n",
        "df_test = df.loc['2000-04-20':]\n",
        "df_train.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "XTB-3nwd27Kv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1189.408002180309"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# your code here\n",
        "df = pd.read_csv(\"share_prices.csv\", parse_dates=True, index_col=0)\n",
        "df.index.freq = \"D\"  # set as \"daily\" frequency; observations are recorded every day. Removes a warning in statsmodels\n",
        "\n",
        "# Training and testing set\n",
        "df_train = df.loc[:'2000-04-19']\n",
        "df_test = df.loc['2000-04-20':]\n",
        "\n",
        "\n",
        "model = AutoReg(df_train, lags=28)\n",
        "model_fit = model.fit()\n",
        "\n",
        "predictions = model_fit.predict(start='2000-04-20', end='2000-05-02')\n",
        "\n",
        "mean_squared_error(df_test, predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ziijh9jr27Kv"
      },
      "source": [
        "---\n",
        "**Problem 6.** In this question, **use auto-regression model to find the best value of `lags` from 1 to 10 both inclusive**. Here **best** is defined to be **\"minimum mean squared error on the validation set\"**. The dataset has been split as follows:\n",
        "\n",
        "- Training set: January 1st, 2000 to April 1st, 2000\n",
        "- Validation set: from April 2nd, 2000 to May 2nd, 2000.\n",
        "\n",
        "The validation set has 31 observations, which provides a more fair assessment of different values of `maxlag`; there are at least 20 predictions that are solely created by the model and don't use any \"free\" historical data.\n",
        "\n",
        "*Gold star question: Plot the predictions for the best `lags`. Would you use this model to buy stocks?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "cNG4uXsh27Kv"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"share_prices.csv\", parse_dates=True, index_col=0)\n",
        "df.index.freq = \"D\"  # set as \"daily\" frequency; observations are recorded every day. Removes a warning in statsmodels\n",
        "\n",
        "# Training and validation set\n",
        "df_train = df.loc[:'2000-04-01']\n",
        "df_val = df.loc['2000-04-02':]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "iExzJZ7k27Kw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# your code here\n",
        "\n",
        "def evaluate_model(maxlag, train, val):\n",
        "    model = AutoReg(train, lags=maxlag)\n",
        "    model_fit = model.fit()\n",
        "    predictions = model_fit.predict(start=val.index[0], end=val.index[-1])\n",
        "    mse = mean_squared_error(val, predictions)\n",
        "    return mse\n",
        "\n",
        "best_mse = float('inf')\n",
        "best_maxlag = None\n",
        "for maxlag in range(1, 11):\n",
        "    mse = evaluate_model(maxlag, df_train, df_val)\n",
        "    if mse < best_mse:\n",
        "        best_mse = mse\n",
        "        best_maxlag = maxlag\n",
        "\n",
        "best_maxlag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "A = np.array([[17,  6, 20, -2, -9,  3, 18, 14,  2],\n",
        "       [ 2, 17,  6, 20, -2, -9,  3, 18, 14],\n",
        "       [14,  2, 17,  6, 20, -2, -9,  3, 18],\n",
        "       [18, 14,  2, 17,  6, 20, -2, -9,  3],\n",
        "       [ 3, 18, 14,  2, 17,  6, 20, -2, -9],\n",
        "       [-9,  3, 18, 14,  2, 17,  6, 20, -2],\n",
        "       [-2, -9,  3, 18, 14,  2, 17,  6, 20],\n",
        "       [20, -2, -9,  3, 18, 14,  2, 17,  6],\n",
        "       [ 6, 20, -2, -9,  3, 18, 14,  2, 17]])\n",
        "\n",
        "\n",
        "for i in range(1, len(A)):\n",
        "    if not np.array_equal(A[i], np.roll(A[i-1], 1)):\n",
        "        print(\"No\")\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "B = np.array([[17,  2, 14, 18,  3, -9, -2, 20,  6,  5,  2],\n",
        "              [18,  3, -9, -2, 20,  6,  5,  2, 17,  2, 14],\n",
        "              [-2, 20,  6,  5,  2, 17,  2, 14, 18,  3, -9],\n",
        "              [ 5,  2, 17,  2, 14, 18,  3, -9, -2, 20,  6],\n",
        "              [ 2, 14, 18,  3, -9, -2, 20,  6,  5,  2, 17],\n",
        "              [ 3, -9, -2, 20,  6,  5,  2, 17,  2, 14, 18],\n",
        "              [20,  6,  5,  2, 17,  2, 14, 18,  3, -9, -2],\n",
        "              [ 2, 17,  2, 14, 18,  3, -9, -2, 20,  6,  5],\n",
        "              [14, 18,  3, -9, -2, 20,  6,  5,  2, 17,  2],\n",
        "              [-9, -2, 20,  6,  5,  2, 17,  2, 14, 18,  3],\n",
        "              [ 6,  5,  2, 17,  2, 14, 18,  3, -9, -2, 20]])\n",
        "\n",
        "\n",
        "for i in range(1, len(B)):\n",
        "    if not np.array_equal(B[i], np.roll(B[i-1], -3)):\n",
        "        print(\"No\")\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9650"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = 'Feature engineering involves creating new features from existing data to improve model performance.'\n",
        "\n",
        "total_sum = 0\n",
        "for char in s:\n",
        "    total_sum += ord(char)\n",
        "total_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = 'AUC-ROC curve measures the performance of binary classification models.'\n",
        "total_sum = 0\n",
        "\n",
        "for char in s:\n",
        "    total_sum += ord(char)\n",
        "result = total_sum % 23\n",
        "\n",
        "result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['algorithms',\n",
              " 'analysis',\n",
              " 'analytics',\n",
              " 'analyze',\n",
              " 'and',\n",
              " 'anomalies',\n",
              " 'are',\n",
              " 'assesses',\n",
              " 'auc',\n",
              " 'balances',\n",
              " 'based',\n",
              " 'better',\n",
              " 'between',\n",
              " 'bias',\n",
              " 'big',\n",
              " 'binary',\n",
              " 'but',\n",
              " 'choose',\n",
              " 'classification',\n",
              " 'clustering',\n",
              " 'combine',\n",
              " 'complexity',\n",
              " 'create',\n",
              " 'creating',\n",
              " 'cross',\n",
              " 'crucial',\n",
              " 'curve',\n",
              " 'data',\n",
              " 'datasets',\n",
              " 'dimensionality',\n",
              " 'efficiently',\n",
              " 'enables',\n",
              " 'engineering',\n",
              " 'enhances',\n",
              " 'ensemble',\n",
              " 'essential',\n",
              " 'evaluates',\n",
              " 'evaluating',\n",
              " 'experiments',\n",
              " 'exploratory',\n",
              " 'extracting',\n",
              " 'feature',\n",
              " 'features',\n",
              " 'focuses',\n",
              " 'for',\n",
              " 'forecasting',\n",
              " 'from',\n",
              " 'future',\n",
              " 'generalization',\n",
              " 'group',\n",
              " 'handle',\n",
              " 'helps',\n",
              " 'historical',\n",
              " 'human',\n",
              " 'hypothesis',\n",
              " 'identifies',\n",
              " 'important',\n",
              " 'informative',\n",
              " 'insights',\n",
              " 'language',\n",
              " 'learning',\n",
              " 'machine',\n",
              " 'massive',\n",
              " 'measures',\n",
              " 'methods',\n",
              " 'metrics',\n",
              " 'model',\n",
              " 'modeling',\n",
              " 'models',\n",
              " 'multiple',\n",
              " 'natural',\n",
              " 'new',\n",
              " 'number',\n",
              " 'occurs',\n",
              " 'off',\n",
              " 'outcomes',\n",
              " 'overfitting',\n",
              " 'patterns',\n",
              " 'performance',\n",
              " 'performs',\n",
              " 'play',\n",
              " 'points',\n",
              " 'poorly',\n",
              " 'precision',\n",
              " 'predictions',\n",
              " 'predictive',\n",
              " 'process',\n",
              " 'processing',\n",
              " 'raw',\n",
              " 'recall',\n",
              " 'reduce',\n",
              " 'reduction',\n",
              " 'regression',\n",
              " 'relationships',\n",
              " 'relevant',\n",
              " 'representations',\n",
              " 'roc',\n",
              " 'role',\n",
              " 'selection',\n",
              " 'series',\n",
              " 'significance',\n",
              " 'similar',\n",
              " 'statistical',\n",
              " 'techniques',\n",
              " 'technologies',\n",
              " 'temporal',\n",
              " 'test',\n",
              " 'testing',\n",
              " 'the',\n",
              " 'time',\n",
              " 'together',\n",
              " 'tools',\n",
              " 'trade',\n",
              " 'training',\n",
              " 'understand',\n",
              " 'unseen',\n",
              " 'used',\n",
              " 'validation',\n",
              " 'variables',\n",
              " 'variance',\n",
              " 'visual',\n",
              " 'visualization',\n",
              " 'well',\n",
              " 'when']"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "expressions = [\n",
        "    \"Data analysis is essential for extracting insights from raw data.\",\n",
        "    \"Machine learning algorithms play a crucial role in predictive modeling.\",\n",
        "    \"Statistical modeling helps us understand relationships between variables.\",\n",
        "    \"Predictive analytics enables forecasting future outcomes based on historical data.\",\n",
        "    \"Big data technologies handle massive datasets efficiently.\",\n",
        "    \"Data visualization tools create informative visual representations of data.\",\n",
        "    \"Natural language processing techniques analyze and process human language data.\",\n",
        "    \"Feature engineering enhances model performance by creating new features.\",\n",
        "    \"Exploratory data analysis identifies patterns and anomalies in data.\",\n",
        "    \"Regression analysis is used to model relationships between variables.\",\n",
        "    \"Clustering algorithms group similar data points together.\",\n",
        "    \"Dimensionality reduction techniques reduce the number of features in data.\",\n",
        "    \"Time series analysis focuses on temporal patterns in data.\",\n",
        "    \"Cross-validation assesses model performance on unseen data.\",\n",
        "    \"Ensemble methods combine multiple models for better predictions.\",\n",
        "    \"Hypothesis testing evaluates statistical significance in experiments.\",\n",
        "    \"AUC-ROC curve measures the performance of binary classification models.\",\n",
        "    \"Precision and recall are important metrics for evaluating model performance.\",\n",
        "    \"Overfitting occurs when a model performs well on training data but poorly on test data.\",\n",
        "    \"Bias-variance trade-off balances model complexity and generalization.\",\n",
        "    \"Feature selection methods choose relevant features for model training.\"\n",
        "]\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "text = ' '.join(expressions)\n",
        "words = re.findall(r'\\b\\w{3,}\\b', text.lower())\n",
        "unique_words = sorted(set(words))\n",
        "word_to_id = {word: idx for idx, word in enumerate(unique_words)}\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "unique_words, word_to_id = find_unique_words(expressions)\n",
        "\n",
        "text = ' '.join(expressions)\n",
        "\n",
        "words = re.findall(r'\\b\\w{3,}\\b', text.lower())\n",
        "nth_word = words[61]\n",
        "\n",
        "unique_words.index(nth_word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['AUC', 'Bias', 'Big', 'Clustering', 'Cross', 'Data', 'Dimensionality', 'Ensemble', 'Exploratory', 'Feature', 'Hypothesis', 'Machine', 'Natural', 'Overfitting', 'Precision', 'Predictive', 'ROC', 'Regression', 'Statistical', 'Time', 'algorithms', 'analysis', 'analytics', 'analyze', 'and', 'anomalies', 'are', 'assesses', 'balances', 'based', 'better', 'between', 'binary', 'but', 'choose', 'classification', 'combine', 'complexity', 'create', 'creating', 'crucial', 'curve', 'data', 'datasets', 'efficiently', 'enables', 'engineering', 'enhances', 'essential', 'evaluates', 'evaluating', 'experiments', 'extracting', 'features', 'focuses', 'for', 'forecasting', 'from', 'future', 'generalization', 'group', 'handle', 'helps', 'historical', 'human', 'identifies', 'important', 'informative', 'insights', 'language', 'learning', 'massive', 'measures', 'methods', 'metrics', 'model', 'modeling', 'models', 'multiple', 'new', 'number', 'occurs', 'off', 'outcomes', 'patterns', 'performance', 'performs', 'play', 'points', 'poorly', 'predictions', 'predictive', 'process', 'processing', 'raw', 'recall', 'reduce', 'reduction', 'relationships', 'relevant', 'representations', 'role', 'selection', 'series', 'significance', 'similar', 'statistical', 'techniques', 'technologies', 'temporal', 'test', 'testing', 'the', 'together', 'tools', 'trade', 'training', 'understand', 'unseen', 'used', 'validation', 'variables', 'variance', 'visual', 'visualization', 'well', 'when']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "expressions = [\n",
        "    \"Data analysis is essential for extracting insights from raw data.\",\n",
        "    \"Machine learning algorithms play a crucial role in predictive modeling.\",\n",
        "    \"Statistical modeling helps us understand relationships between variables.\",\n",
        "    \"Predictive analytics enables forecasting future outcomes based on historical data.\",\n",
        "    \"Big data technologies handle massive datasets efficiently.\",\n",
        "    \"Data visualization tools create informative visual representations of data.\",\n",
        "    \"Natural language processing techniques analyze and process human language data.\",\n",
        "    \"Feature engineering enhances model performance by creating new features.\",\n",
        "    \"Exploratory data analysis identifies patterns and anomalies in data.\",\n",
        "    \"Regression analysis is used to model relationships between variables.\",\n",
        "    \"Clustering algorithms group similar data points together.\",\n",
        "    \"Dimensionality reduction techniques reduce the number of features in data.\",\n",
        "    \"Time series analysis focuses on temporal patterns in data.\",\n",
        "    \"Cross-validation assesses model performance on unseen data.\",\n",
        "    \"Ensemble methods combine multiple models for better predictions.\",\n",
        "    \"Hypothesis testing evaluates statistical significance in experiments.\",\n",
        "    \"AUC-ROC curve measures the performance of binary classification models.\",\n",
        "    \"Precision and recall are important metrics for evaluating model performance.\",\n",
        "    \"Overfitting occurs when a model performs well on training data but poorly on test data.\",\n",
        "    \"Bias-variance trade-off balances model complexity and generalization.\",\n",
        "    \"Feature selection methods choose relevant features for model training.\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "text = ' '.join(expressions)\n",
        "words = re.findall(r'\\b\\w{3,}\\b', text)\n",
        "unique_words = sorted(set(words))\n",
        "word_to_id = {word: idx for idx, word in enumerate(unique_words)}\n",
        "print(unique_words)\n",
        "\n",
        "nth_word = words[62-1] \n",
        "unique_words.index(nth_word)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.992"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "word_frequency = {word: 0 for word in unique_words}\n",
        "for expression in expressions:\n",
        "    for word in unique_words:\n",
        "        if word in expression.lower(): \n",
        "            word_frequency[word] += 1\n",
        "length_expressions = len(expressions)\n",
        "expression_A = {}\n",
        "for word, frequency in word_frequency.items():\n",
        "    A = math.log((length_expressions + 1) / (frequency + 1))\n",
        "    expression_A[word] = A\n",
        "expression_B = {word: round(A + 1, 3) for word, A in expression_A.items()}\n",
        "    \n",
        "word = unique_words[73]\n",
        "expression_B[word]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
