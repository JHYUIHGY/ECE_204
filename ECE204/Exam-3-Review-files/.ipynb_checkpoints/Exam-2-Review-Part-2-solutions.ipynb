{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm 2 Review (Part II)\n",
    "\n",
    "## ECE204 Data Science & Engineering\n",
    "### This notebook contains practice questions for Midterm 2. The following topics are covered:\n",
    "- KMeans Clustering\n",
    "- PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Question 1\n",
    "> This question involves KMeans clustering.\n",
    "> We have a cleaner version of the blobs dataset from last part, `blobs_2.csv`. <br>\n",
    "> Your objective is to run KMeans (with random_state=42) on the data, and find the folloing: <br>\n",
    "> 1. The total number of points that belong to each cluster.\n",
    "> 2. Which cluster center is the point [2, 2] closest to? Find the distance of this point to the nearest cluster center. <br>\n",
    ">\n",
    ">\n",
    "> Solve the problem using the following steps:\n",
    "> 1. Run KMeans on the data with `n_clusters=2`, since it looks like there are 2 distinct groups. Make sure to set `random_state=42`.\n",
    "> 2. Predict the cluster assignment for each data point. Using this, find out the number of points that belong to each cluster.\n",
    "> 3. Find the distance of the point [2, 2] to each cluster center, and report the smallest distance. (Alternatively, you could predict the cluster assignment for this point, and then find the distance to that point only.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('blobs_2.csv')\n",
    "df.plot.scatter(x=\"foo\", y=\"bar\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution - Part 1\n",
    "km = KMeans(n_clusters=2, random_state = 42)\n",
    "y_hat =km.fit(df).predict(df)\n",
    "\n",
    "#y = km.fit_predict(df)\n",
    "print(\"No. of points in cluster 1:\", sum(y_hat))\n",
    "print(\"No. of points in cluster 2:\", len(y_hat) - sum(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Solution - Part 2\n",
    "import numpy as np\n",
    "\n",
    "new_point = np.array([2, 2])\n",
    "\n",
    "distances = [np.linalg.norm(c - new_point) for c in km.cluster_centers_]\n",
    "\n",
    "print(\"Distance of point (2, 2) to each cluster center:\", distances)\n",
    "print(\"Prediction for point (2, 2) : \", km.predict([[2, 2]]))\n",
    "\n",
    "# Both prediction and calculation of distance to cluster centers tell us that [2, 2] lies closer to the Second Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x=\"foo\", y=\"bar\", c= y_hat, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "> Read in the dataset `cars.csv`. We want approximate this dataset with only 2 dimensions. You can use Principal component analysis (PCA) to do dimensionality reduction.\n",
    ">\n",
    "> Perform PCA on this dataset. While doing this, only numeric columns will be allowed. Your objective is to find out the following:\n",
    "\n",
    "> 1. ** What is the standard deviation of each column in the original dataset?**\n",
    "> 2. ** What is the total explained variance after reducing the data to 2 dimensions **\n",
    "> 3. Each column has a different scale, and very different standard deviations. **Standarize the data, and then find out what is the total explained variance after reducing the data to 2 dimensions**. Reason about why there is a difference in the two results.\n",
    ">\n",
    "> Solve the problem using the following steps: <br>\n",
    "> 1. Filter out the non-numeric columns out of the dataframe. We will perform PCA on numeric data only.\n",
    "> 2. Find the standard deviation of each column. (Recall that there is a pandas method for this)\n",
    "> 3. Run PCA on the data to reduce it to 2 dimensions, and check the total explained vairance.\n",
    "> 4. Use `StandardScaler` to normalize the original data, and then run PCA to reduce the dimensions, and check the total explained variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"cars.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Acceleration', 'Cylinders', 'Displacement', 'Horsepower',\n",
    "       'Miles_per_Gallon', 'Weight_in_lbs', 'Year']\n",
    "\n",
    "df = df[cols]\n",
    "X = df.values\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing PCA, without any preprocessing\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_low = pca.fit_transform(df)\n",
    "(pca.explained_variance_ratio_).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing PCA, but preprocess to normalize each feature\n",
    "\n",
    "norm = StandardScaler()\n",
    "X_norm = norm.fit_transform(df) # Let's call the normalized dataset Y\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_norm_low = pca.fit_transform(X_norm)\n",
    "\n",
    "(pca.explained_variance_ratio_).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Acceleration', 'Cylinders', 'Displacement', 'Horsepower', 'Miles_per_Gallon', 'Weight_in_lbs', 'Year']]\n",
    "X = df.values\n",
    "pca = PCA(n_components = 2)\n",
    "X2 = pca.fit_transform(X)\n",
    "print(pca.explained_variance_ratio_.sum())\n",
    "\n",
    "norm = StandardScaler()\n",
    "X_norm = norm.fit_transform(X) \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_norm_low = pca.fit_transform(X_norm)\n",
    "\n",
    "print(pca.explained_variance_ratio_.sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
