{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5226454388736"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER = \"7316717653133062491922511967442657474235534919493496983520312774506326239578318016984801869112293272858615607891129494954595017379583319528532088055111254069874715852386305071569329096329522744304355766896648950445244523161731856403098711121722383113622296934233803081353362766142828064444866452387493035890729629049156044077239071381051585930796086670172427121883948797908792274921901699720888093776657273330010533678812202354218097512545405947522435258490771167055601360483958644670632441572215539753697817977846174064955149290862569321978468622482839722413756570560574902614079729686524145351004748216637048440317949000889524345065854122758866688116427171479924442928230863465674813919123162824586178664583591245665294765456828489128831426076900422421902267105562632111110937054421750694165896040807198403850962455444362981230987879927244284909188845801561660979191338754992005240636859125607176060588611646710940507754100225698315520005593572972571636269561882670428252483600823257530420752963450\"\n",
    "\n",
    "def max_product_squares(number, n):\n",
    "    max_product = 0\n",
    "    for i in range(len(number) - n + 1):\n",
    "        product = 1\n",
    "        for digit in number[i:i+n]:\n",
    "            product *= int(digit) ** 2\n",
    "        max_product = max(max_product, product)\n",
    "    return max_product\n",
    "\n",
    "result = max_product_squares(NUMBER, 7)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def caesar_cipher(text):\n",
    "    encrypted_text = ''\n",
    "    shift = 1\n",
    "    for sentence in text.split('.'):\n",
    "        for char in sentence:\n",
    "            if char.isalpha():\n",
    "                offset = ord('a') if char.islower() else ord('A')\n",
    "                encrypted_char = chr((ord(char) - offset + shift) % 26 + offset)\n",
    "                encrypted_text += encrypted_char\n",
    "            else:\n",
    "                encrypted_text += char\n",
    "        shift += 1\n",
    "    return encrypted_text\n",
    "\n",
    "def count_j(text):\n",
    "    return text.count('j')\n",
    "\n",
    "def count(text):\n",
    "    encrypted_text = caesar_cipher(text)\n",
    "    j_count = count_j(encrypted_text)\n",
    "    return j_count\n",
    "\n",
    "text = \"\"\"\n",
    "Data science plus deep neural networks help us study big data. Big data is huge, complex, diverse datasets from sources like web, commerce, health, science. Big data gives us issues plus chances, such as:\n",
    "\n",
    "- Volume: Big data is big, which needs methods plus tools to store, process, study the data, like systems, cloud, plus computing.\n",
    "- Variety: Big data is different, which needs methods plus tools to use plus mix the data, like cleaning, preprocessing, plus transforming.\n",
    "- Velocity: Big data is quick, which needs methods plus tools to get, stream, plus process the data, like ingestion, streaming, plus online.\n",
    "- Veracity: Big data is low, which needs methods plus tools to check, fix, plus correct the data, like quality, cleansing, plus imputation.\n",
    "\n",
    "Data science plus deep neural networks use techniques plus models to get useful patterns plus insights from big data. Some of the techniques plus models we use:\n",
    "\n",
    "- Mining plus exploring: This is finding plus getting info plus knowledge from big data, like rules, patterns, groups, plus outliers.\n",
    "- Engineering plus picking: This is making plus choosing good plus helpful features from big data, like extraction, transformation, scaling, plus picking.\n",
    "- Reducing plus learning: This is cutting plus showing the big plus complex big data in simple plus low space, like analysis, encoders, plus embeddings.\n",
    "- Deep learning plus networks: This is building plus teaching complex plus nonlinear models to learn from big data, like networks, convolutional, recurrent, transformers, plus generative.\n",
    "\n",
    "Data science plus deep neural networks study big data for purposes plus fields, such as:\n",
    "\n",
    "- Solving NP-hard issues: NP-hard issues need time to solve or check, like salesman, knapsack, plus problem. Data science plus deep neural networks solve these issues by using methods, like genetic, cooling, plus learning, to find solutions in time.\n",
    "- Solving key issues: Key issues need us to do something for society plus people, like climate, health, plus school. Data science plus deep neural networks solve these issues by using methods, like regression, grouping, clustering, plus systems, to give insights, guesses, plus tips for doing plus picking.\n",
    "\n",
    "Data science plus deep neural networks grow plus improve quickly, which give us chances plus issues. Data science plus deep neural networks need us to be creative, curious, plus keen. Data science plus deep neural networks help us do plus improve with data.\n",
    "\"\"\"\n",
    "\n",
    "result = count(text)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "r = lambda _: random.randint(1, 300)\n",
    "random.seed(0)\n",
    "events = [(r(0), r(0)+1) for i in range(500)]\n",
    "events = list(map(lambda t: (t[0], t[0] + random.randint(2, 5)) if t[0] >= t[1] else t, events))\n",
    "\n",
    "\n",
    "def max_events(events):\n",
    "    events.sort(key=lambda x: x[1])\n",
    "    \n",
    "    max_events = 0\n",
    "    last_end_time = float('-inf')\n",
    "    \n",
    "    for event in events:\n",
    "        start_time, end_time = event\n",
    "        if start_time >= last_end_time:\n",
    "            max_events += 1\n",
    "            last_end_time = end_time\n",
    "    \n",
    "    return max_events\n",
    "\n",
    "max_events(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def caesar_cipher(text):\n",
    "    encrypted_text = ''\n",
    "    shift = 1\n",
    "    for sentence in text.split('.'):\n",
    "        for char in sentence:\n",
    "            if char.isalpha():\n",
    "                offset = ord('a') if char.islower() else ord('A')\n",
    "                encrypted_char = chr((ord(char) - offset + shift) % 26 + offset)\n",
    "                encrypted_text += encrypted_char\n",
    "            else:\n",
    "                encrypted_text += char\n",
    "        shift += 1\n",
    "    return encrypted_text\n",
    "\n",
    "def count_j(text):\n",
    "    return text.count('j')\n",
    "\n",
    "def count(text):\n",
    "    encrypted_text = caesar_cipher(text)\n",
    "    j_count = count_j(encrypted_text)\n",
    "    return j_count\n",
    "\n",
    "text = \"\"\"\n",
    "Data science plus deep neural networks help us study big data. Big data is huge, complex, diverse datasets from sources like web, commerce, health, science. Big data gives us issues plus chances, such as:\n",
    "\n",
    "- Volume: Big data is big, which needs methods plus tools to store, process, study the data, like systems, cloud, plus computing.\n",
    "- Variety: Big data is different, which needs methods plus tools to use plus mix the data, like cleaning, preprocessing, plus transforming.\n",
    "- Velocity: Big data is quick, which needs methods plus tools to get, stream, plus process the data, like ingestion, streaming, plus online.\n",
    "- Veracity: Big data is low, which needs methods plus tools to check, fix, plus correct the data, like quality, cleansing, plus imputation.\n",
    "\n",
    "Data science plus deep neural networks use techniques plus models to get useful patterns plus insights from big data. Some of the techniques plus models we use:\n",
    "\n",
    "- Mining plus exploring: This is finding plus getting info plus knowledge from big data, like rules, patterns, groups, plus outliers.\n",
    "- Engineering plus picking: This is making plus choosing good plus helpful features from big data, like extraction, transformation, scaling, plus picking.\n",
    "- Reducing plus learning: This is cutting plus showing the big plus complex big data in simple plus low space, like analysis, encoders, plus embeddings.\n",
    "- Deep learning plus networks: This is building plus teaching complex plus nonlinear models to learn from big data, like networks, convolutional, recurrent, transformers, plus generative.\n",
    "\n",
    "Data science plus deep neural networks study big data for purposes plus fields, such as:\n",
    "\n",
    "- Solving NP-hard issues: NP-hard issues need time to solve or check, like salesman, knapsack, plus problem. Data science plus deep neural networks solve these issues by using methods, like genetic, cooling, plus learning, to find solutions in time.\n",
    "- Solving key issues: Key issues need us to do something for society plus people, like climate, health, plus school. Data science plus deep neural networks solve these issues by using methods, like regression, grouping, clustering, plus systems, to give insights, guesses, plus tips for doing plus picking.\n",
    "\n",
    "Data science plus deep neural networks grow plus improve quickly, which give us chances plus issues. Data science plus deep neural networks need us to be creative, curious, plus keen. Data science plus deep neural networks help us do plus improve with data.\n",
    "\"\"\"\n",
    "\n",
    "result = count(text)\n",
    "result\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def has_matching_pairs(latex_text):\n",
    "    lines = latex_text.split('\\n')\n",
    "    \n",
    "    begin_stack = []\n",
    "    \n",
    "    for line in lines:\n",
    "        comment_index = line.find('%')\n",
    "        if comment_index != -1:\n",
    "            line = line[:comment_index]\n",
    "        \n",
    "        begin_indices = [pos for pos, char in enumerate(line) if line[pos:pos+6] == '\\\\begin']\n",
    "        end_indices = [pos for pos, char in enumerate(line) if line[pos:pos+4] == '\\\\end']\n",
    "        \n",
    "        for begin_index in begin_indices:\n",
    "            begin_stack.append(begin_index)\n",
    "        \n",
    "        for end_index in end_indices:\n",
    "            if len(begin_stack) == 0:\n",
    "                return False\n",
    "            else:\n",
    "                begin_stack.pop()\n",
    "    \n",
    "    if len(begin_stack) == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "latex_text = r\"\"\"\n",
    "\\begin{equation}\n",
    "    E = mc^2\n",
    "\\end{equation}\n",
    "\n",
    "% Some comment\n",
    "\\begin{align}\n",
    "    a &= b \\\\\n",
    "    c &= d\n",
    "\\end{align}\n",
    "\n",
    "% Another comment\n",
    "\\end{document}\n",
    "\"\"\"\n",
    "has_matching_pairs(latex_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_text2 = r\"$\\textbf{Data Science}$ is the interdisciplinary field that combines techniques from $\\textit{statistics}$, $\\textit{computer science}$, and $\\textit{domain knowledge}$ to extract $\\textit{insights}$ and $\\textit{knowledge}$ from $\\textit{structured}$ and $\\textit{unstructured data}$. It involves $\\textit{data cleaning}$, $\\textit{exploratory data analysis}$, $\\textit{machine learning}$, and $\\textit{visualization}$.\"\n",
    "has_matching_pairs(latex_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_text3 = r\"\"\"\n",
    "Data science is the interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data. Data science can be applied to various domains, such as natural sciences, social sciences, engineering, business, and health care. Some of the common techniques and tools used in data science are:\n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item \\textbf{Data collection and preprocessing}: This involves gathering, cleaning, and transforming data from various sources, such as databases, web pages, sensors, surveys, etc. Data preprocessing aims to make the data suitable for further analysis and modeling. For example, one can use the following equation to normalize the data:\n",
    "\n",
    "    \\begin{equation}\n",
    "        x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "    \\end{equation}\n",
    "\n",
    "    where $x$ is the original data and $x'$ is the normalized data. Another example is to use the following block to impute missing values:\n",
    "\n",
    "    \\begin{myCom}\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imp = SimpleImputer(strategy='mean')\n",
    "        X = imp.fit_transform(X)\n",
    "    \\end{myCom}\n",
    "\n",
    "    where $X$ is the data matrix with missing values and $imp$ is the imputer object.\n",
    "    \\item \\textbf{Data analysis and visualization}: This involves exploring, summarizing, and presenting the data using descriptive statistics, graphs, charts, maps, etc. Data analysis and visualization help to understand the patterns, trends, and relationships in the data. For example, one can use the following equation to calculate the mean of the data:\n",
    "\n",
    "    \\begin{equation}\n",
    "        \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "    \\end{equation}\n",
    "\n",
    "    where $n$ is the number of data points and $x_i$ is the $i$-th data point. Another example is to use the following block to plot a histogram of the data:\n",
    "\n",
    "    \\begin{myCom}\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.hist(x, bins=10)\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Histogram of Data')\n",
    "        plt.show()\n",
    "    \\end{myCom}\n",
    "\n",
    "    where $x$ is the data vector and $plt$ is the plotting object.\n",
    "    \\item \\textbf{Data modeling and machine learning}: This involves building and applying mathematical and computational models to the data, such as regression, classification, clustering, dimensionality reduction, etc. Data modeling and machine learning help to infer, predict, and optimize the outcomes of interest from the data. For example, one can use the following equation to perform linear regression on the data:\n",
    "\n",
    "    \\begin{equation}\n",
    "        y = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "    \\end{equation}\n",
    "\n",
    "    where $y$ is the dependent variable, $x$ is the independent variable, $\\beta_0$ and $\\beta_1$ are the coefficients, and $\\epsilon$ is the error term. Another example is to use the following block to train a neural network on the data:\n",
    "\n",
    "    \\begin{myCom}\n",
    "        import tensorflow as tf\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(10, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(X, y, epochs=10, batch_size=32)\n",
    "    \\end{myCom}\n",
    "\n",
    "    where $X$ and $y$ are the input and output data, $model$ is the neural network object, and $tf$ is the TensorFlow library.\n",
    "    \\item \\textbf{Data communication and reporting}: This involves communicating and disseminating the results and insights from the data to various stakeholders, such as decision-makers, customers, researchers, etc. Data communication and reporting help to inform, persuade, and educate the audience about the data-driven findings and recommendations. For example, one can use the following equation to measure the accuracy of a classifier:\n",
    "\n",
    "    \\begin{equation}\n",
    "        \\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "    \\end{equation}\n",
    "\n",
    "    Another example is to use the following itemize command to list the main conclusions from the data analysis:\n",
    "\n",
    "    \\begin{itemize}\n",
    "        \\item The data shows a positive correlation between $x$ and $y$.\n",
    "        \\item The data has a skewed distribution with a long tail to the right.\n",
    "        \\item The data can be clustered into three distinct groups based on $z$.\n",
    "    \\end{itemize}\n",
    "\\end{itemize}\n",
    "\n",
    "Data science is a rapidly evolving and expanding field that requires a combination of skills and knowledge from different disciplines, such as mathematics, statistics, computer science, domain expertise, and communication. Data science has the potential to generate significant value and impact for various sectors and society at large.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "has_matching_pairs(latex_text3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bonding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
